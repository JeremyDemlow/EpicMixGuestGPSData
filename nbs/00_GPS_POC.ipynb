{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epic Mix Guest GPS POC\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp corez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## snowflake_poc_table_grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def snowflake_poc_table_grab(sf_connection):\n",
    "    \"Grab the data from a snowflake to allow for play on GPS Data\"\n",
    "    return f'Hello {sf_connection}!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Need a suggestion!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowflake_poc_table_grab('Need a suggestion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "test_eq(snowflake_poc_table_grab(\"Need a suggestion\"), 'Hello Need a suggestion!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark Read Azure GPS Data For Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 13:03:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import dbutils as dbutils\n",
    "# from pyspark.conf import SparkConf\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# import pyspark.sql.functions as func\n",
    "# from pyspark.sql.types import * \n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql import functions, Window\n",
    "# from pyspark.sql.types import *\n",
    "# from math import radians, cos, sin, asin, sqrt\n",
    "# from functools import reduce\n",
    "# import pandas as pd\n",
    "# from matplotlib import pyplot as plt\n",
    "# import os\n",
    "\n",
    "# ACCOUNT_KEY = \"EzJJ19jNqJfABvCc/39ygXONjvpjZYdoQJ9XtpGpoy/Mcv22lWDCCeKkiD8wXUJYtCnTMbOyVz2Q6SkUyoX4aA==\"\n",
    "# BLOB_CONTAINER = \"lumiplan-data\"\n",
    "# BLOB_ACCOUNT = \"vaildtscadls\"\n",
    "# MOUNT_PATH = \"/mnt/lumiplan-data/\"\n",
    "# AZURE_ACCOUNT = \"fs.azure.account.key.{BLOB_ACCOUNT}.blob.core.windows.net\"\n",
    "# MASETER = \"local[*]\"  #\"spark://spark-master-svc:7077\"\n",
    "# APP_NAME = \"GPS_SAMPLE_EXTRACT\"\n",
    "# SOURCE = f\"wasbs://{BLOB_CONTAINER}@{BLOB_ACCOUNT}.blob.core.windows.net/\"\n",
    "# # SOURCE = f\"abfss://{BLOB_CONTAINER}@{BLOB_ACCOUNT}.blob.core.windows.net/\"\n",
    "# conf = SparkConf().set(\"spark.driver.memory\", \"60g\")\n",
    "# sc = SparkContext(\"local[*]\", APP_NAME, conf=conf)\n",
    "# sc.setLogLevel(\"ERROR\")\n",
    "# spark = SparkSession.builder.appName('pytest').getOrCreate()\n",
    "# data_lake_secret = os.environ['DATALAKE_SECRET']\n",
    "# data_lake_account = \"vaildtscadls\"\n",
    "# spark.conf.set(\"fs.azure.account.key.\" + data_lake_account + \".blob.core.windows.net\", data_lake_secret)\n",
    "# spark.conf.set(\"fs.azure.account.key.\" + data_lake_account + \".dfs.core.windows.net\", data_lake_secret)\n",
    "# spark.conf.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file=f\"wasbs://{BLOB_CONTAINER}@{BLOB_ACCOUNT}.dfs.core.windows.net/lumiplan-data/sample-extract/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_schema = StructType([\n",
    "#     StructField('skier_id', IntegerType()),\n",
    "#     StructField('latitude', DoubleType()),\n",
    "#     StructField('longitude', DoubleType()),\n",
    "#     StructField('timestamp', TimestampType()),\n",
    "# ])\n",
    "\n",
    "# df = spark.read.parquet(\n",
    "#     file,\n",
    "#     schema=geo_schema,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparkConf = SparkConf() \\\n",
    "#     .setAppName(APP_NAME) \\\n",
    "#     .setMaster(MASETER) \\\n",
    "#     .set(AZURE_ACCOUNT, ACCOUNT_KEY) \\\n",
    "#     .set(\"spark.jars.packages\",\n",
    "#          \"org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-storage:8.6.6\")\n",
    "\n",
    "# spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# def spark():\n",
    "\n",
    "#     conf = SparkConf().set(\"spark.driver.memory\", \"60g\")\n",
    "#     sc = SparkContext(\"local[*]\", \"pytest\", conf=conf)\n",
    "#     sc.setLogLevel(\"ERROR\")\n",
    "#     spark = SparkSession.builder.appName('pytest').getOrCreate()\n",
    "#     data_lake_secret = os.environ['DATALAKE_SECRET']\n",
    "#     data_lake_account = \"vaildtscadls\"\n",
    "#     spark.conf.set(\"fs.azure.account.key.\" + data_lake_account + \".blob.core.windows.net\", data_lake_secret)\n",
    "#     spark.conf.set(\"fs.azure.account.key.\" + data_lake_account + \".dfs.core.windows.net\", data_lake_secret)\n",
    "\n",
    "#     yield spark\n",
    "#     sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
