{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8d24c-4e52-49eb-918d-63d1de7cc422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/07 14:29:44 WARN Utils: Your hostname, Jeremys-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.30 instead (on interface en0)\n",
      "22/10/07 14:29:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/10/07 14:29:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/07 14:29:45 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.\n",
      "22/10/07 14:29:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/07 14:29:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923ee2b-f7d0-4344-967a-452394032582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import line_magic, line_cell_magic, Magics, magics_class\n",
    "\n",
    "@magics_class\n",
    "class DatabricksConnectMagics(Magics):\n",
    "\n",
    "    @line_cell_magic\n",
    "    def sql(self, line, cell=None):\n",
    "        if cell and line:\n",
    "            raise ValueError(\"Line must be empty for cell magic\", line)\n",
    "        try:\n",
    "            from autovizwidget.widget.utils import display_dataframe\n",
    "        except ImportError:\n",
    "            print(\"Please run `pip install autovizwidget` to enable the visualization widget.\")\n",
    "        display_dataframe = lambda x: x\n",
    "        return display_dataframe(self.get_spark().sql(cell or line).toPandas())\n",
    "\n",
    "    def get_spark(self):\n",
    "        user_ns = get_ipython().user_ns\n",
    "        if \"spark\" in user_ns:\n",
    "            return user_ns[\"spark\"]\n",
    "        else:\n",
    "            from pyspark.sql import SparkSession\n",
    "        user_ns[\"spark\"] = SparkSession.builder.getOrCreate()\n",
    "        return user_ns[\"spark\"]\n",
    "\n",
    "ip = get_ipython()\n",
    "ip.register_magics(DatabricksConnectMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea0ce6-5658-45b2-95d1-c1b940786700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks-connect==10.4.*\n",
      "  Downloading databricks-connect-10.4.12.tar.gz (292.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.4/292.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.1\n",
      "  Using cached py4j-0.10.9.1-py2.py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: six in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from databricks-connect==10.4.*) (1.16.0)\n",
      "Building wheels for collected packages: databricks-connect\n",
      "  Building wheel for databricks-connect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-connect: filename=databricks_connect-10.4.12-py2.py3-none-any.whl size=293143619 sha256=8fcc51521766415fb658995c9bf6b8e570a632aed338669ea92f938dcb00cd2f\n",
      "  Stored in directory: /Users/jeremydemlow/Library/Caches/pip/wheels/15/25/f2/555b21a283411f2bdd6387014978bb224979a23002447b49bc\n",
      "Successfully built databricks-connect\n",
      "Installing collected packages: py4j, databricks-connect\n",
      "Successfully installed databricks-connect-10.4.12 py4j-0.10.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"databricks-connect==10.4.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87c475-da5a-4649-9884-994e4be74f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current configuration is:\n",
      "* Databricks Host: https://adb-76307882911213.13.azuredatabricks.net/\n",
      "* Databricks Token: dapi3450f53f5a2075b1d5ce26a1da28f1a6-3\n",
      "* Cluster ID: 1007-210902-t57d8fmt\n",
      "* Org ID: 76307882911213\n",
      "* Port: 15001\n",
      "Set new config values (leave input empty to accept default):\n",
      "Databricks Host [https://adb-76307882911213.13.azuredatabricks.net/]: ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeremydemlow/miniforge3/envs/fastai/bin/databricks-connect\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages/pyspark/databricks_connect.py\", line 281, in main\n",
      "    configure()\n",
      "  File \"/Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages/pyspark/databricks_connect.py\", line 127, in configure\n",
      "    new_host = input().strip()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! databricks-connect configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fa06f-b005-408d-a6d2-a5e86d894afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[1;32m      2\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68ce6b-e4ad-48c1-9eaf-8c59e018ff0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dbutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdbutils\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dbutils'"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions, Window\n",
    "from pyspark.sql.types import *\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "import dbutils\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fa799-7985-481c-8341-2255722320b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a55c31-afd7-4063-9222-3c426b23be6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m BLOB_ACCOUNT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvaildtscadls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m MOUNT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/lumiplan-data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n\u001b[1;32m      8\u001b[0m   source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwasbs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLOB_CONTAINER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLOB_ACCOUNT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.blob.core.windows.net/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m   mount_point \u001b[38;5;241m=\u001b[39m MOUNT_PATH,\n\u001b[1;32m     10\u001b[0m   extra_configs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.key.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLOB_ACCOUNT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.blob.core.windows.net\u001b[39m\u001b[38;5;124m\"\u001b[39m:ACCOUNT_KEY\n\u001b[1;32m     12\u001b[0m   }\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "ACCOUNT_KEY = \"EzJJ19jNqJfABvCc/39ygXONjvpjZYdoQJ9XtpGpoy/Mcv22lWDCCeKkiD8wXUJYtCnTMbOyVz2Q6SkUyoX4aA==\"\n",
    "BLOB_CONTAINER = \"lumiplan-data\"\n",
    "BLOB_ACCOUNT = \"vaildtscadls\"\n",
    "MOUNT_PATH = \"/mnt/lumiplan-data/\"\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = f\"wasbs://{BLOB_CONTAINER}@{BLOB_ACCOUNT}.blob.core.windows.net/\",\n",
    "  mount_point = MOUNT_PATH,\n",
    "  extra_configs = {\n",
    "    f\"fs.azure.account.key.{BLOB_ACCOUNT}.blob.core.windows.net\":ACCOUNT_KEY\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1fc2b-8d6d-4e5e-b523-a556031cf313",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://adb-76307882911213.13.azuredatabricks.net/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
