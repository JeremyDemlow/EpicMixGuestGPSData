{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8d24c-4e52-49eb-918d-63d1de7cc422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/07 14:29:44 WARN Utils: Your hostname, Jeremys-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.30 instead (on interface en0)\n",
      "22/10/07 14:29:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/10/07 14:29:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/07 14:29:45 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.\n",
      "22/10/07 14:29:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/07 14:29:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923ee2b-f7d0-4344-967a-452394032582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import line_magic, line_cell_magic, Magics, magics_class\n",
    "\n",
    "@magics_class\n",
    "class DatabricksConnectMagics(Magics):\n",
    "\n",
    "    @line_cell_magic\n",
    "    def sql(self, line, cell=None):\n",
    "        if cell and line:\n",
    "            raise ValueError(\"Line must be empty for cell magic\", line)\n",
    "        try:\n",
    "            from autovizwidget.widget.utils import display_dataframe\n",
    "        except ImportError:\n",
    "            print(\"Please run `pip install autovizwidget` to enable the visualization widget.\")\n",
    "        display_dataframe = lambda x: x\n",
    "        return display_dataframe(self.get_spark().sql(cell or line).toPandas())\n",
    "\n",
    "    def get_spark(self):\n",
    "        user_ns = get_ipython().user_ns\n",
    "        if \"spark\" in user_ns:\n",
    "            return user_ns[\"spark\"]\n",
    "        else:\n",
    "            from pyspark.sql import SparkSession\n",
    "        user_ns[\"spark\"] = SparkSession.builder.getOrCreate()\n",
    "        return user_ns[\"spark\"]\n",
    "\n",
    "ip = get_ipython()\n",
    "ip.register_magics(DatabricksConnectMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea0ce6-5658-45b2-95d1-c1b940786700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks-connect==10.4.*\n",
      "  Downloading databricks-connect-10.4.12.tar.gz (292.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m292.4/292.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.1\n",
      "  Using cached py4j-0.10.9.1-py2.py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: six in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from databricks-connect==10.4.*) (1.16.0)\n",
      "Building wheels for collected packages: databricks-connect\n",
      "  Building wheel for databricks-connect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-connect: filename=databricks_connect-10.4.12-py2.py3-none-any.whl size=293143619 sha256=8fcc51521766415fb658995c9bf6b8e570a632aed338669ea92f938dcb00cd2f\n",
      "  Stored in directory: /Users/jeremydemlow/Library/Caches/pip/wheels/15/25/f2/555b21a283411f2bdd6387014978bb224979a23002447b49bc\n",
      "Successfully built databricks-connect\n",
      "Installing collected packages: py4j, databricks-connect\n",
      "Successfully installed databricks-connect-10.4.12 py4j-0.10.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"databricks-connect==10.4.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87c475-da5a-4649-9884-994e4be74f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* PySpark is installed at /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages/pyspark\n",
      "* Checking SPARK_HOME\n",
      "* Checking java version\n",
      "openjdk version \"11.0.16.1\" 2022-08-12\n",
      "OpenJDK Runtime Environment Homebrew (build 11.0.16.1+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 11.0.16.1+0, mixed mode)\n",
      "WARNING: Java versions >8 are not supported by this SDK\n",
      "* Testing scala command\n",
      "22/10/07 14:35:00 WARN Utils: Your hostname, Jeremys-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.30 instead (on interface en0)\n",
      "22/10/07 14:35:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/10/07 14:35:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/07 14:35:03 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.\n",
      "22/10/07 14:35:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/07 14:35:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/07 14:35:04 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "Spark context Web UI available at http://192.168.1.30:4043                      \n",
      "Spark context available as 'sc' (master = local[*], app id = local-1665178504231).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.0-SNAPSHOT\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.14 (OpenJDK 64-Bit Server VM, Java 11.0.16.1)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\n",
      "scala> \n",
      "\n",
      "scala>             import com.databricks.service.SparkClientManager\n",
      "import com.databricks.service.SparkClientManager\n",
      "\n",
      "tServerSparkConf   val serverConf = SparkClientManager.getForCurrentSession().ge \n",
      "serverConf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@3ea5f324\n",
      "\n",
      ".databricks.pyspark.enableProcessIsolation\")rverConf                 .get(\"spark \n",
      "processIsolation: String = false\n",
      "\n",
      "scala>             if (!processIsolation.toBoolean) {\n",
      "     |                 spark.range(100).reduce((a,b) => Long.box(a + b))\n",
      "     |             } else {\n",
      "     |                 spark.range(99*100/2).count()\n",
      "     |             }\n",
      "View job details at https://adb-76307882911213.13.azuredatabricks.net/?o=76307882911213#/setting/clusters/1007-210902-t57d8fmt/sparkUi\n",
      "View job details at https://adb-76307882911213.13.azuredatabricks.net/?o=76307882911213#/setting/clusters/1007-210902-t57d8fmt/sparkUi\n",
      "res0: Any = 4950\n",
      "\n",
      "scala>             \n",
      "     | \n",
      "scala> :quit\n",
      "\n",
      "* Simple Scala test passed\n",
      "* Testing python command\n",
      "22/10/07 14:35:13 WARN Utils: Your hostname, Jeremys-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.30 instead (on interface en0)\n",
      "22/10/07 14:35:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/10/07 14:35:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/07 14:35:14 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.\n",
      "22/10/07 14:35:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/07 14:35:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/07 14:35:14 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "View job details at https://adb-76307882911213.13.azuredatabricks.net/?o=76307882911213#/setting/clusters/1007-210902-t57d8fmt/sparkUi\n",
      "* Simple PySpark test passed                                       (0 + 4) / 10]\n",
      "* Testing dbutils.fs\n",
      "[FileInfo(path='dbfs:/FileStore/', name='FileStore/', size=0, modificationTime=1657120924000), FileInfo(path='dbfs:/databricks-datasets/', name='databricks-datasets/', size=0, modificationTime=0), FileInfo(path='dbfs:/databricks-results/', name='databricks-results/', size=0, modificationTime=0), FileInfo(path='dbfs:/mnt/', name='mnt/', size=0, modificationTime=1657130557000), FileInfo(path='dbfs:/user/', name='user/', size=0, modificationTime=1657580841000)]\n",
      "* Simple dbutils test passed\n",
      "* All tests passed.\n"
     ]
    }
   ],
   "source": [
    "! databricks-connect test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fa06f-b005-408d-a6d2-a5e86d894afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dbx\n",
      "  Downloading dbx-0.7.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.24.0 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from dbx) (2.28.1)\n",
      "Collecting aiohttp>=3.8.1\n",
      "  Downloading aiohttp-3.8.3-cp38-cp38-macosx_11_0_arm64.whl (337 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typer[all]==0.6.1\n",
      "  Downloading typer-0.6.1-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: Jinja2>=2.11.2 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from dbx) (3.1.2)\n",
      "Collecting pathspec>=0.9.0\n",
      "  Using cached pathspec-0.10.1-py3-none-any.whl (27 kB)\n",
      "Collecting mlflow-skinny<=2.0.0,>=1.28.0\n",
      "  Downloading mlflow_skinny-1.29.0-py3-none-any.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting retry<1.0.0,>=0.9.2\n",
      "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
      "Collecting cryptography<39.0.0,>=3.3.1\n",
      "  Using cached cryptography-38.0.1-cp36-abi3-macosx_10_10_universal2.whl (5.3 MB)\n",
      "Collecting rich==12.5.1\n",
      "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m235.6/235.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=8.1.0 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from dbx) (8.1.3)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from dbx) (6.0)\n",
      "Collecting databricks-cli<0.18,>=0.17\n",
      "  Using cached databricks_cli-0.17.3-py3-none-any.whl\n",
      "Collecting cookiecutter<3.0.0,>=1.7.2\n",
      "  Downloading cookiecutter-2.1.1-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: watchdog>=2.1.0 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from dbx) (2.1.9)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from dbx) (1.9.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from rich==12.5.1->dbx) (2.13.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m918.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from rich==12.5.1->dbx) (4.4.0)\n",
      "Collecting colorama<0.5.0,>=0.4.3\n",
      "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
      "Collecting shellingham<2.0.0,>=1.3.0\n",
      "  Downloading shellingham-1.5.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp38-cp38-macosx_11_0_arm64.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from aiohttp>=3.8.1->dbx) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from aiohttp>=3.8.1->dbx) (22.1.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp38-cp38-macosx_11_0_arm64.whl (35 kB)\n",
      "Collecting jinja2-time>=0.2.0\n",
      "  Downloading jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\n",
      "Collecting binaryornot>=0.4.4\n",
      "  Downloading binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting python-slugify>=4.0.0\n",
      "  Downloading python_slugify-6.1.2-py2.py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from cryptography<39.0.0,>=3.3.1->dbx) (1.15.1)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Downloading oauthlib-3.2.1-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from databricks-cli<0.18,>=0.17->dbx) (1.16.0)\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from Jinja2>=2.11.2->dbx) (2.1.1)\n",
      "Requirement already satisfied: packaging<22 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from mlflow-skinny<=2.0.0,>=1.28.0->dbx) (21.3)\n",
      "Collecting importlib-metadata!=4.7.0,<5,>=3.7.0\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Downloading sqlparse-0.4.3-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<5,>=3.12.0\n",
      "  Downloading protobuf-4.21.7-cp37-abi3-macosx_10_9_universal2.whl (484 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m484.0/484.0 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitpython<4,>=2.1.0\n",
      "  Downloading GitPython-3.1.28-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m182.5/182.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle<3\n",
      "  Using cached cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: pytz<2023 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from mlflow-skinny<=2.0.0,>=1.28.0->dbx) (2022.4)\n",
      "Requirement already satisfied: entrypoints<1 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from mlflow-skinny<=2.0.0,>=1.28.0->dbx) (0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->dbx) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->dbx) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->dbx) (3.4)\n",
      "Collecting py<2.0.0,>=1.4.26\n",
      "  Using cached py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: decorator>=3.4.2 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from retry<1.0.0,>=0.9.2->dbx) (5.1.1)\n",
      "Collecting chardet>=3.0.2\n",
      "  Downloading chardet-5.0.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from cffi>=1.12->cryptography<39.0.0,>=3.3.1->dbx) (2.21)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<5,>=3.7.0->mlflow-skinny<=2.0.0,>=1.28.0->dbx) (3.8.1)\n",
      "Collecting arrow\n",
      "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from packaging<22->mlflow-skinny<=2.0.0,>=1.28.0->dbx) (3.0.9)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m160.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /Users/jeremydemlow/miniforge3/envs/fastai/lib/python3.8/site-packages (from arrow->jinja2-time>=0.2.0->cookiecutter<3.0.0,>=1.7.2->dbx) (2.8.2)\n",
      "Installing collected packages: text-unidecode, commonmark, typer, tabulate, sqlparse, smmap, shellingham, rich, python-slugify, pyjwt, py, protobuf, pathspec, oauthlib, multidict, importlib-metadata, frozenlist, colorama, cloudpickle, chardet, async-timeout, yarl, retry, gitdb, databricks-cli, cryptography, binaryornot, arrow, aiosignal, jinja2-time, gitpython, aiohttp, mlflow-skinny, cookiecutter, dbx\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.4.2\n",
      "    Uninstalling typer-0.4.2:\n",
      "      Successfully uninstalled typer-0.4.2\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 5.0.0\n",
      "    Uninstalling importlib-metadata-5.0.0:\n",
      "      Successfully uninstalled importlib-metadata-5.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.4.1 requires typer<0.5.0,>=0.3.0, but you have typer 0.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.8.3 aiosignal-1.2.0 arrow-1.2.3 async-timeout-4.0.2 binaryornot-0.4.4 chardet-5.0.0 cloudpickle-2.2.0 colorama-0.4.5 commonmark-0.9.1 cookiecutter-2.1.1 cryptography-38.0.1 databricks-cli-0.17.3 dbx-0.7.6 frozenlist-1.3.1 gitdb-4.0.9 gitpython-3.1.28 importlib-metadata-4.13.0 jinja2-time-0.2.0 mlflow-skinny-1.29.0 multidict-6.0.2 oauthlib-3.2.1 pathspec-0.10.1 protobuf-4.21.7 py-1.11.0 pyjwt-2.5.0 python-slugify-6.1.2 retry-0.9.2 rich-12.5.1 shellingham-1.5.0 smmap-5.0.0 sqlparse-0.4.3 tabulate-0.9.0 text-unidecode-1.3 typer-0.6.1 yarl-1.8.1\n",
      "\u001b[1;31m[\u001b[0m\u001b[31mdbx\u001b[0m\u001b[1;31m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2022\u001b[0m-\u001b[1;36m10\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m14:36:27\u001b[0m.\u001b[1;36m416\u001b[0m\u001b[1m]\u001b[0m 洫ｱ\u001b[31mDatabricks\u001b[0m e\u001b[31mX\u001b[0mtensions aka \u001b[31mdbx\u001b[0m, version ~> \u001b[1;32m0.7\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install dbx --upgrade\n",
    "! dbx --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68ce6b-e4ad-48c1-9eaf-8c59e018ff0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dbutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdbutils\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dbutils'"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions, Window\n",
    "from pyspark.sql.types import *\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "import dbutils\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fa799-7985-481c-8341-2255722320b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a55c31-afd7-4063-9222-3c426b23be6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m BLOB_ACCOUNT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvaildtscadls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m MOUNT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/lumiplan-data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n\u001b[1;32m      8\u001b[0m   source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwasbs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLOB_CONTAINER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLOB_ACCOUNT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.blob.core.windows.net/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m   mount_point \u001b[38;5;241m=\u001b[39m MOUNT_PATH,\n\u001b[1;32m     10\u001b[0m   extra_configs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.key.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLOB_ACCOUNT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.blob.core.windows.net\u001b[39m\u001b[38;5;124m\"\u001b[39m:ACCOUNT_KEY\n\u001b[1;32m     12\u001b[0m   }\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "ACCOUNT_KEY = \"EzJJ19jNqJfABvCc/39ygXONjvpjZYdoQJ9XtpGpoy/Mcv22lWDCCeKkiD8wXUJYtCnTMbOyVz2Q6SkUyoX4aA==\"\n",
    "BLOB_CONTAINER = \"lumiplan-data\"\n",
    "BLOB_ACCOUNT = \"vaildtscadls\"\n",
    "MOUNT_PATH = \"/mnt/lumiplan-data/\"\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = f\"wasbs://{BLOB_CONTAINER}@{BLOB_ACCOUNT}.blob.core.windows.net/\",\n",
    "  mount_point = MOUNT_PATH,\n",
    "  extra_configs = {\n",
    "    f\"fs.azure.account.key.{BLOB_ACCOUNT}.blob.core.windows.net\":ACCOUNT_KEY\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1fc2b-8d6d-4e5e-b523-a556031cf313",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://adb-76307882911213.13.azuredatabricks.net/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
